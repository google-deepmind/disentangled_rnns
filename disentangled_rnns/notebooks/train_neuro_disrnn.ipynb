{
  "cells": [
    {
      "metadata": {
        "id": "iQ_8aK7dji9u"
      },
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/google-deepmind/disentangled_rnns/blob/main/disentangled_rnns/notebooks/train_neuro_disrnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "nK-s9L5PLq2f"
      },
      "cell_type": "code",
      "source": [
        "# Install disentangled_rnns repo from github\n",
        "!git clone https://github.com/google-deepmind/disentangled_rnns\n",
        "%cd disentangled_rnns\n",
        "!pip install .\n",
        "%cd ..\n",
        "\n",
        "import optax\n",
        "import numpy as np\n",
        "\n",
        "from disentangled_rnns.library import rnn_utils\n",
        "from disentangled_rnns.library import neuro_disrnn\n",
        "from disentangled_rnns.library import checkpoint_utils\n",
        "from disentangled_rnns.library import two_armed_bandits_w_dopamine"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "ApA1YfVGz9Uq"
      },
      "cell_type": "markdown",
      "source": [
        "# Define a dataset"
      ]
    },
    {
      "metadata": {
        "id": "MIhLKbgHPYmQ"
      },
      "cell_type": "code",
      "source": [
        "# @title Dataset Selection\n",
        "\n",
        "dataset_configs = {\n",
        "    \"q_learning_w_dopamine\":{\n",
        "        \"getter\": two_armed_bandits_w_dopamine.get_q_learning_with_dopamine_dataset,\n",
        "        \"kwargs\": {\"n_trials\": 100, \"n_sessions\": 100},\n",
        "        \"penalties\": {\n",
        "            \"latent_penalty\": 1e-2,\n",
        "            \"choice_net_latent_penalty\": 1e-4,\n",
        "            \"update_net_latent_penalty\": 2e-3,\n",
        "            \"neural_activity_net_latent_penalty\": 1e-4,\n",
        "            \"update_net_obs_penalty\": 1e-5,\n",
        "        },\n",
        "    },\n",
        "    \"reward_seeking\":{\n",
        "        \"getter\": two_armed_bandits_w_dopamine.get_reward_seeking_with_dopamine_dataset,\n",
        "        \"kwargs\": {\"n_trials\":100, \"n_sessions\": 100},\n",
        "        \"penalties\": {\n",
        "            \"latent_penalty\": 1e-3,\n",
        "            \"choice_net_latent_penalty\": 1e-4,\n",
        "            \"update_net_latent_penalty\": 2e-3,\n",
        "            \"neural_activity_net_latent_penalty\": 1e-4,\n",
        "            \"update_net_obs_penalty\": 1e-5,\n",
        "        }\n",
        "\n",
        "    },\n",
        "}\n",
        "\n",
        "dataset_name = \"q_learning_w_dopamine\"  # @param [\"q_learning_w_dopamine\", \"reward_seeking\"]\n",
        "dataset_config = dataset_configs[dataset_name]\n",
        "dataset = dataset_config[\"getter\"](**dataset_config[\"kwargs\"])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "caSlZS4OR0PK"
      },
      "cell_type": "code",
      "source": [
        "dataset_train, dataset_eval = rnn_utils.split_dataset(dataset, 2)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "ONzEfURn0DU4"
      },
      "cell_type": "markdown",
      "source": [
        "# Define and train RNN"
      ]
    },
    {
      "metadata": {
        "id": "Ie3QyClIBxhU"
      },
      "cell_type": "code",
      "source": [
        "disrnn_w_neural_activity_config = neuro_disrnn.DisRnnWNeuralActivityConfig(\n",
        "      # Dataset related\n",
        "      obs_size=2,  # Choice, reward\n",
        "      output_size=2,  # Choose left / choose right\n",
        "      x_names=dataset.x_names,\n",
        "      y_names=dataset.y_names,\n",
        "      # Network architecture\n",
        "      latent_size=7,\n",
        "      update_net_n_units_per_layer=16,\n",
        "      update_net_n_layers=4,\n",
        "      choice_net_n_units_per_layer=2,\n",
        "      choice_net_n_layers=2,\n",
        "      neural_activity_net_n_units_per_layer=4,\n",
        "      neural_activity_net_n_layers=2,\n",
        "      activation='leaky_relu',\n",
        "      # Penalties\n",
        "      noiseless_mode=False,\n",
        "      latent_penalty=np.nan,\n",
        "      choice_net_latent_penalty=np.nan,\n",
        "      update_net_latent_penalty=np.nan,\n",
        "      neural_activity_net_latent_penalty=np.nan,\n",
        "  )\n",
        "\n",
        "for penalty_name, penalty_value in dataset_config[\"penalties\"].items():\n",
        "    setattr(disrnn_w_neural_activity_config, penalty_name, penalty_value)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "OULn6VOf0l-R"
      },
      "cell_type": "code",
      "source": [
        "# Initial training in noiseless mode\n",
        "\n",
        "likelihood_weight = 0.5 # @param {type: \"slider\",min:0, max: 1, step: 0.1}\n",
        "params, opt_state, losses = rnn_utils.train_network(\n",
        "   lambda: neuro_disrnn.HkNeuroDisentangledRNN(disrnn_w_neural_activity_config),\n",
        "    dataset_train,\n",
        "    dataset_eval,\n",
        "    opt = optax.adam(1e-3),\n",
        "    loss=\"penalized_hybrid\",\n",
        "    loss_param={'likelihood_weight': likelihood_weight, 'penalty_scale': 1.0},\n",
        "    n_steps=0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "5ohhgHpbAkBD"
      },
      "cell_type": "code",
      "source": [
        "# RUN THIS CELL AND THE ONES BELOW IT MANY TIMES\n",
        "# Running this cell repeatedly continues to train the same network.\n",
        "# The cells below make plots documenting what's going on in your network\n",
        "# If you'd like to reinitialize the network, re-run the above cell\n",
        "# Try tweaking the bottleneck parameters as you train, to get a feel for how they affect things\n",
        "\n",
        "\n",
        "# Usually 15,000 steps in total should be sufficient.\n",
        "n_steps = 15_000\n",
        "params, opt_state, losses = rnn_utils.train_network(\n",
        "    lambda: neuro_disrnn.HkNeuroDisentangledRNN(disrnn_w_neural_activity_config),\n",
        "    dataset_train,\n",
        "    dataset_eval,\n",
        "    loss=\"penalized_hybrid\",\n",
        "    params=params,\n",
        "    opt_state=opt_state,\n",
        "    opt = optax.adam(1e-3),\n",
        "    loss_param = {'likelihood_weight': likelihood_weight, 'penalty_scale': 1.0},\n",
        "    n_steps=n_steps,\n",
        "    do_plot = True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "ClNkwuMZoh3T"
      },
      "cell_type": "code",
      "source": [
        "# Plot the open/closed state of the bottlenecks. Ideally neural activity bottlenecks\n",
        "# should stay closed as we are not training the neural activity readout right now.\n",
        "\n",
        "_=neuro_disrnn.plot_bottlenecks(params, disrnn_w_neural_activity_config, sort_latents=False)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "xGT3ldciHUr_"
      },
      "cell_type": "code",
      "source": [
        "# Plot the choice rule\n",
        "neuro_disrnn.plot_choice_rule(params, disrnn_w_neural_activity_config)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "N-g0Uzcxq8VU"
      },
      "cell_type": "code",
      "source": [
        "# Plot the update rules\n",
        "neuro_disrnn.plot_update_rules(params, disrnn_w_neural_activity_config)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "HHlubM69rPW1"
      },
      "cell_type": "code",
      "source": [
        "# Plot neural activity rules\n",
        "_ = neuro_disrnn.plot_neural_activity_rules(params, disrnn_w_neural_activity_config, axis_lim=0.8)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
