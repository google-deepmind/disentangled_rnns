{
  "cells": [
    {
      "metadata": {
        "id": "iQ_8aK7dji9u"
      },
      "cell_type": "markdown",
      "source": [
        "\u003ca href=\"https://colab.research.google.com/github/google-deepmind/disentangled_rnns/blob/main/disentangled_rnns/notebooks/train_neuro_disrnn.ipynb\" target=\"_parent\"\u003e\u003cimg src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/\u003e\u003c/a\u003e"
      ]
    },
    {
      "metadata": {
        "id": "nK-s9L5PLq2f"
      },
      "cell_type": "code",
      "source": [
        "# Install disentangled_rnns repo from github\n",
        "!git clone https://github.com/google-deepmind/disentangled_rnns\n",
        "%cd disentangled_rnns\n",
        "!pip install .\n",
        "%cd ..\n",
        "\n",
        "import optax\n",
        "import numpy as np\n",
        "\n",
        "from disentangled_rnns.library import rnn_utils\n",
        "from disentangled_rnns.library import neuro_disrnn\n",
        "from disentangled_rnns.library import checkpoint_utils\n",
        "from disentangled_rnns.library import two_armed_bandits_w_dopamine"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "ApA1YfVGz9Uq"
      },
      "cell_type": "markdown",
      "source": [
        "# Define a dataset"
      ]
    },
    {
      "metadata": {
        "id": "MIhLKbgHPYmQ"
      },
      "cell_type": "code",
      "source": [
        "# @title Dataset Selection\n",
        "\n",
        "dataset_configs = {\n",
        "    \"q_learning_w_dopamine\":{\n",
        "        \"getter\": two_armed_bandits_w_dopamine.get_q_learning_with_dopamine_dataset,\n",
        "        \"kwargs\": {\"n_trials\": 100, \"n_sessions\": 100},\n",
        "        \"penalties\": {\n",
        "            \"latent_penalty\": 1e-3,\n",
        "            \"choice_net_latent_penalty\": 1e-4,\n",
        "            \"update_net_latent_penalty\": 2e-3,\n",
        "            \"neural_activity_net_latent_penalty\": 1e-4,\n",
        "            \"update_net_obs_penalty\": 1e-5,\n",
        "        },\n",
        "    },\n",
        "    \"reward_seeking\":{\n",
        "        \"getter\": two_armed_bandits_w_dopamine.get_reward_seeking_with_dopamine_dataset,\n",
        "        \"kwargs\": {\"n_trials\":100, \"n_sessions\": 100},\n",
        "        \"penalties\": {\n",
        "            \"latent_penalty\": 1e-3,\n",
        "            \"choice_net_latent_penalty\": 1e-4,\n",
        "            \"update_net_latent_penalty\": 2e-3,\n",
        "            \"neural_activity_net_latent_penalty\": 1e-4,\n",
        "            \"update_net_obs_penalty\": 1e-5,\n",
        "        }\n",
        "\n",
        "    },\n",
        "}\n",
        "\n",
        "dataset_name = \"q_learning_w_dopamine\"  # @param [\"q_learning_w_dopamine\", \"reward_seeking\"]\n",
        "dataset_config = dataset_configs[dataset_name]\n",
        "dataset = dataset_config[\"getter\"](**dataset_config[\"kwargs\"])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "caSlZS4OR0PK"
      },
      "cell_type": "code",
      "source": [
        "dataset_train, dataset_eval = rnn_utils.split_dataset(dataset, 2)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "ONzEfURn0DU4"
      },
      "cell_type": "markdown",
      "source": [
        "# Define and train RNN"
      ]
    },
    {
      "metadata": {
        "id": "Ie3QyClIBxhU"
      },
      "cell_type": "code",
      "source": [
        "disrnn_w_neural_activity_config = neuro_disrnn.DisRnnWNeuralActivityConfig(\n",
        "      # Dataset related\n",
        "      obs_size=2,  # Choice, reward\n",
        "      output_size=2,  # Choose left / choose right\n",
        "      x_names=dataset.x_names,\n",
        "      y_names=dataset.y_names,\n",
        "      # Network architecture\n",
        "      latent_size=7,\n",
        "      update_net_n_units_per_layer=16,\n",
        "      update_net_n_layers=4,\n",
        "      choice_net_n_units_per_layer=2,\n",
        "      choice_net_n_layers=2,\n",
        "      neural_activity_net_n_units_per_layer=4,\n",
        "      neural_activity_net_n_layers=2,\n",
        "      activation='leaky_relu',\n",
        "      # Penalties\n",
        "      noiseless_mode=False,\n",
        "      latent_penalty=np.nan,\n",
        "      choice_net_latent_penalty=np.nan,\n",
        "      update_net_latent_penalty=np.nan,\n",
        "      neural_activity_net_latent_penalty=np.nan,\n",
        "  )\n",
        "\n",
        "for penalty_name, penalty_value in dataset_config[\"penalties\"].items():\n",
        "    setattr(disrnn_w_neural_activity_config, penalty_name, penalty_value)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "OULn6VOf0l-R"
      },
      "cell_type": "code",
      "source": [
        "# Initial training in noiseless mode\n",
        "\n",
        "params, opt_state, losses = rnn_utils.train_network(\n",
        "   lambda: neuro_disrnn.HkNeuroDisentangledRNN(disrnn_w_neural_activity_config),\n",
        "    dataset_train,\n",
        "    dataset_eval,\n",
        "    opt = optax.adam(1e-3),\n",
        "    loss=\"penalized_hybrid\",\n",
        "    loss_param={'likelihood_weight': 1.0, 'penalty_scale': 1.0},\n",
        "    n_steps=0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "5ohhgHpbAkBD"
      },
      "cell_type": "code",
      "source": [
        "# RUN THIS CELL AND THE ONES BELOW IT MANY TIMES\n",
        "# Running this cell repeatedly continues to train the same network.\n",
        "# The cells below make plots documenting what's going on in your network\n",
        "# If you'd like to reinitialize the network, re-run the above cell\n",
        "# Try tweaking the bottleneck parameters as you train, to get a feel for how they affect things\n",
        "\n",
        "\n",
        "# Usually 20,000 steps in total should be sufficient.\n",
        "n_steps = 20_000\n",
        "\n",
        "params, opt_state, losses = rnn_utils.train_network(\n",
        "    lambda: neuro_disrnn.HkNeuroDisentangledRNN(disrnn_w_neural_activity_config),\n",
        "    dataset_train,\n",
        "    dataset_eval,\n",
        "    loss=\"penalized_hybrid\",\n",
        "    params=params,\n",
        "    opt_state=opt_state,\n",
        "    opt = optax.adam(1e-3),\n",
        "    loss_param = {'likelihood_weight': 1.0, 'penalty_scale': 1.0},\n",
        "    n_steps=n_steps,\n",
        "    do_plot = True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "ClNkwuMZoh3T"
      },
      "cell_type": "code",
      "source": [
        "# Plot the open/closed state of the bottlenecks. Ideally neural activity bottlenecks\n",
        "# should stay closed as we are not training the neural activity readout right now.\n",
        "\n",
        "_=neuro_disrnn.plot_bottlenecks(params, disrnn_w_neural_activity_config, sort_latents=False)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "xGT3ldciHUr_"
      },
      "cell_type": "code",
      "source": [
        "# Plot the choice rule\n",
        "neuro_disrnn.plot_choice_rule(params, disrnn_w_neural_activity_config)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "N-g0Uzcxq8VU"
      },
      "cell_type": "code",
      "source": [
        "# Plot the update rules\n",
        "neuro_disrnn.plot_update_rules(params, disrnn_w_neural_activity_config)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "vzqo6M_Nqz16"
      },
      "cell_type": "markdown",
      "source": [
        "# FineTune Neural Activity ReadOut"
      ]
    },
    {
      "metadata": {
        "id": "s2A-pkMXyRah"
      },
      "cell_type": "code",
      "source": [
        "# We keep rest of the network frozen and train just the dopamine readout.\n",
        "\n",
        "trainable_param_names = ['neural_activity_net']\n",
        "\n",
        "opt = checkpoint_utils.get_optimizer_with_frozen_params(\n",
        "    optax.adam(1e-3), params, trainable_param_names)\n",
        "\n",
        "# Open Dopamine bottlenecks again.\n",
        "params[\"hk_neuro_disentangled_rnn\"][\"neural_activity_net_sigma_params\"] = np.random.uniform(\n",
        "    low=-0.2, high=0.2, size=(disrnn_w_neural_activity_config.latent_size +2,))\n",
        "\n",
        "params[\"hk_neuro_disentangled_rnn\"][\"neural_activity_net_multipliers\"] = np.ones(\n",
        "    shape=(disrnn_w_neural_activity_config.latent_size +2,))\n",
        "\n",
        "disrnn_w_neural_activity_config.dopamine_net_penalty = 1e-3\n",
        "# Train one step to initialize everything.\n",
        "params, opt_state, losses = rnn_utils.train_network(\n",
        "   lambda: neuro_disrnn.HkDisentangledRnn(disrnn_w_neural_activity_config),\n",
        "    dataset_train,\n",
        "    dataset_eval,\n",
        "    opt = opt,\n",
        "    loss=\"penalized_hybrid\",\n",
        "    loss_param={'likelihood_weight': 0.0, 'penalty_scale': 1.0},\n",
        "    params=params,\n",
        "    n_steps=0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "VcZZg8n5rG63"
      },
      "cell_type": "code",
      "source": [
        "# Run this cell many times to continue training the network\n",
        "# The network is now only trained on the dopamine readout.\n",
        "# Usually 5,000-6,000 steps should be sufficient.\n",
        "n_steps = 5_000\n",
        "\n",
        "params, _, _ = rnn_utils.train_network(\n",
        "    lambda: neuro_disrnn.HkNeuroDisentangledRNN(disrnn_w_neural_activity_config),\n",
        "    dataset_train,\n",
        "    dataset_eval,\n",
        "    loss=\"penalized_hybrid\",\n",
        "    params=params,\n",
        "    opt_state=opt_state,\n",
        "    opt = opt,\n",
        "    loss_param = {'likelihood_weight': 0.0, 'penalty_scale': 1.0},\n",
        "    n_steps=n_steps,\n",
        "    do_plot = True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "kr4XvmqSrIp-"
      },
      "cell_type": "code",
      "source": [
        "# Plot the open/closed state of the bottlenecks, we should see the neural activity\n",
        "# bottlenecks open up and stay open.\n",
        "neuro_disrnn.plot_bottlenecks(params, disrnn_w_neural_activity_config, sort_latents=False)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "HHlubM69rPW1"
      },
      "cell_type": "code",
      "source": [
        "# Plot neural activity rules\n",
        "neuro_disrnn.plot_neural_activity_rules(params, disrnn_w_neural_activity_config, axis_lim=0.8)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook3_tpu",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/neuroexp/disrnn/notebooks/train_single_disrnn.ipynb?workspaceId=kevinjmiller:disentangled_rnns::citc",
          "timestamp": 1746727499294
        },
        {
          "file_id": "1b5VOqHaVDOJ3fAW2E853NBQbSu2Yi-CP",
          "timestamp": 1727798409618
        },
        {
          "file_id": "1xgFbsQ34Of-WBTEQM_Hf7Di7N9YpRmdR",
          "timestamp": 1726760254895
        },
        {
          "file_id": "1IuwwEfCic7w3NsyVoVPtZSQCzrvTgh_X",
          "timestamp": 1696507812638
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
