{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoeFT9RskVFW"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google-deepmind/disentangled_rnns/blob/main/disentangled_rnns/notebooks/train_single_disrnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSevIZJxda86"
      },
      "outputs": [],
      "source": [
        "# Install disentangled_rnns repo\n",
        "!pip install disentangled_rnns\n",
        "\n",
        "# Import the things we need\n",
        "import optax\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from disentangled_rnns.library import rnn_utils\n",
        "from disentangled_rnns.library import get_datasets\n",
        "from disentangled_rnns.library import disrnn\n",
        "from disentangled_rnns.library import plotting\n",
        "from disentangled_rnns.library import two_armed_bandits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApA1YfVGz9Uq"
      },
      "source": [
        "# Define a dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBz5BKGDwDfL"
      },
      "outputs": [],
      "source": [
        "dataset = get_datasets.get_q_learning_dataset(n_sessions=500,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caSlZS4OR0PK"
      },
      "outputs": [],
      "source": [
        "dataset_train, dataset_eval = rnn_utils.split_dataset(dataset, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONzEfURn0DU4"
      },
      "source": [
        "# Define and train RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zalEsicS0RxY"
      },
      "outputs": [],
      "source": [
        "disrnn_config = disrnn.DisRnnConfig(\n",
        "      # Dataset related\n",
        "      obs_size=2,  # Choice, reward\n",
        "      output_size=2,  # Choose left / choose right\n",
        "      x_names=dataset.x_names,\n",
        "      y_names=dataset.y_names,\n",
        "      # Network architecture\n",
        "      latent_size=5,\n",
        "      update_net_n_units_per_layer=16,\n",
        "      update_net_n_layers=4,\n",
        "      choice_net_n_units_per_layer=4,\n",
        "      choice_net_n_layers=2,\n",
        "      activation='leaky_relu',\n",
        "      # Penalties\n",
        "      noiseless_mode=False,\n",
        "      latent_penalty=1e-5,\n",
        "      choice_net_latent_penalty=1e-5,\n",
        "      update_net_obs_penalty=1e-5,\n",
        "      update_net_latent_penalty=1e-5,\n",
        "      l2_scale=1e-3,\n",
        "  )\n",
        "# Define a config for noiseless, no-penalty training\n",
        "disrnn_config_noiseless = copy.deepcopy(disrnn_config)\n",
        "disrnn_config_noiseless.noiseless_mode = True\n",
        "disrnn_config_noiseless.latent_penalty = 0\n",
        "disrnn_config_noiseless.choice_net_latent_penalty = 0\n",
        "disrnn_config_noiseless.update_net_obs_penalty = 0\n",
        "disrnn_config_noiseless.update_net_latent_penalty = 0\n",
        "disrnn_config_noiseless.l2_scale = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OULn6VOf0l-R"
      },
      "outputs": [],
      "source": [
        "# INITIAL TRAINING IN NOISELESS MODE\n",
        "# Train network in noiseless mode and with no penalty\n",
        "n_steps_noiseless = 1_000  # @param {type: \"integer\"}\n",
        "learning_rate = 1e-2  # @param {type: \"number\"}\n",
        "\n",
        "params, opt_state, _ = rnn_utils.train_network(\n",
        "   lambda: disrnn.HkDisentangledRNN(disrnn_config_noiseless),\n",
        "    training_dataset=dataset_train,\n",
        "    validation_dataset=dataset_eval,\n",
        "    opt = optax.adam(learning_rate=learning_rate),\n",
        "    loss=\"penalized_categorical\",\n",
        "    n_steps=n_steps_noiseless)\n",
        "\n",
        "opt_state = None  # Reset the optimizer state. In the next cell we'll add the bottleneck penalties, which will change the loss function quite a bit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwFLIG_U1Eli"
      },
      "outputs": [],
      "source": [
        "# RUN THIS CELL AND THE ONES BELOW IT MANY TIMES\n",
        "# Running this cell repeatedly continues to train the same network.\n",
        "# The cells below make plots documenting what's going on in your network\n",
        "# If you'd like to reinitialize the network, re-run the above cell\n",
        "# Try tweaking the bottleneck parameters as you train, to get a feel for how they affect things\n",
        "disrnn_config.choice_net_latent_penalty = 1e-3  # @param {type: \"number\"}\n",
        "disrnn_config.update_net_obs_penalty = 1e-3  # @param {type: \"number\"}\n",
        "disrnn_config.update_net_latent_penalty = 1e-3  # @param {type: \"number\"}\n",
        "disrnn_config.latent_penalty = 1e-2   # @param {type: \"number\"}\n",
        "disrnn_config.l2_scale = 1e-3  # @param {type: \"number\"}\n",
        "\n",
        "learning_rate = 1e-3  # @param {type: \"number\"}\n",
        "n_steps = 1_000  # @param {type: \"integer\"}\n",
        "\n",
        "params, opt_state, losses = rnn_utils.train_network(\n",
        "    lambda: disrnn.HkDisentangledRNN(disrnn_config),\n",
        "    dataset_train,\n",
        "    dataset_eval,\n",
        "    loss=\"penalized_categorical\",\n",
        "    params=params,\n",
        "    opt_state=opt_state,\n",
        "    opt = optax.adam(learning_rate=learning_rate),\n",
        "    loss_param = 1,\n",
        "    n_steps=n_steps,\n",
        "    do_plot = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClNkwuMZoh3T"
      },
      "outputs": [],
      "source": [
        "# Plot the open/closed state of the bottlenecks\n",
        "_ = plotting.plot_bottlenecks(params, disrnn_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCTZJafT3X2j"
      },
      "outputs": [],
      "source": [
        "# Plot the choice rule\n",
        "_ = plotting.plot_choice_rule(params, disrnn_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2A-pkMXyRah"
      },
      "outputs": [],
      "source": [
        "# Plot the update rules\n",
        "_ = plotting.plot_update_rules(params, disrnn_config)"
      ]
    },
    {
      "metadata": {
        "id": "LcKwNwah2WoP"
      },
      "cell_type": "code",
      "source": [
        "# Run forward pass on the unseen data\n",
        "eval_data = dataset_eval.get_all()\n",
        "xs_eval, ys_eval = eval_data['xs'], eval_data['ys']\n",
        "network_output, network_states = rnn_utils.eval_network(\n",
        "    lambda: disrnn.HkDisentangledRNN(disrnn_config_noiseless), params, xs_eval)\n",
        "\n",
        "# Compute normalized likelihood\n",
        "logits = network_output[:,:,:2]  # First n_actions elements of network output are the logits (the final one is the penalty)\n",
        "normalized_likelihood = rnn_utils.normalized_likelihood(labels = ys_eval, output_logits=logits)\n",
        "\n",
        "print(f'Normalized likelihood: {100*normalized_likelihood:.2f}%')\n",
        "\n",
        "# Plot network activations on an example session\n",
        "example_session = 0  # @param {type: \"integer\"}\n",
        "\n",
        "choices = xs_eval[:, example_session, 0]\n",
        "rewards = xs_eval[:, example_session, 1]\n",
        "scalars = network_states[:, example_session, :]\n",
        "two_armed_bandits.plot_2ab_sessdata(choices,\n",
        "                                    rewards,\n",
        "                                    scalars=scalars,\n",
        "                                    scalar_types='agent_states',\n",
        "                                    show_legend=False)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook3_tpu",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1b5VOqHaVDOJ3fAW2E853NBQbSu2Yi-CP",
          "timestamp": 1727798409618
        },
        {
          "file_id": "1xgFbsQ34Of-WBTEQM_Hf7Di7N9YpRmdR",
          "timestamp": 1726760254895
        },
        {
          "file_id": "1IuwwEfCic7w3NsyVoVPtZSQCzrvTgh_X",
          "timestamp": 1696507812638
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
